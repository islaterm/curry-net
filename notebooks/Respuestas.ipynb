{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copia de Respuestas_Tarea_5_CC6204_2020 [PUBLICADA]",
   "provenance": [
    {
     "file_id": "1tCAAVMqGCIwy7c1P48IUuypqGVFYCUJs",
     "timestamp": 1609969165839
    },
    {
     "file_id": "1-7Alg0DzdesaVPVuDghwoXbN0pa8ds1t",
     "timestamp": 1521245040538
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zhjpqvcdo5o"
   },
   "source": [
    "# Tarea 5: Redes Recurrentes <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de Respuestas\n",
    "\n",
    "## Nombre: \n",
    "Fecha de entrega: 30 de diciembre de 2020"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "64BkmYga3UN_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968155866,
     "user_tz": 180,
     "elapsed": 5666,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "d292c563-1ba1-4b2a-afc4-bf6b3fa67186"
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Aqui descargamos algunas funciones utiles para resolver la tarea\n",
    "if not os.path.exists('utils.py'):\n",
    "    !wget https://raw.githubusercontent.com/dccuchile/CC6204/master/2020/tareas/tarea5/utils.py -q --show-progress"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   4.10K  --.-KB/s    in 0s      \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KS9LeqZRJ5zn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968160438,
     "user_tz": 180,
     "elapsed": 1337,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    }
   },
   "source": [
    "from utils import extract_text_from_set, extract_text_from_set, tokenize_text \n",
    "from utils import encode_sentences, pad_sequence_with_lengths, pad_sequence_with_images\n",
    "from utils import TextDataset, CaptioningDataset"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNoC8iTiKtg0"
   },
   "source": [
    "# Parte 1: Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnPZTm8HMaNn"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VTiSVBGoK0Xo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968469799,
     "user_tz": 180,
     "elapsed": 307202,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "5c2e4964-0095-4824-e25f-fcb55798aee4"
   },
   "source": [
    "##############################################################################\n",
    "# Todo este código sirve para descargar, preprocesar y dejar los datos\n",
    "# listos para usar después. Después de ejecutar las dos celdas siguientes\n",
    "# tendrás los datos en train_flickr_tripletset y similar para val y test\n",
    "##############################################################################\n",
    "\n",
    "folder_path = './data/flickr8k'\n",
    "if not os.path.exists(f'{folder_path}/images'):\n",
    "    print('*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...')\n",
    "    print('****** Descargando las imágenes...')\n",
    "    !wget https://s06.imfd.cl/04/CC6204/tareas/tarea4/Flickr8k_Dataset.zip -P $folder_path/images -q --show-progress \n",
    "    print('********* Extrayendo las imágenes...\\n  Si te sale mensaje de colab, dale Ignorar\\n')\n",
    "    !unzip -q $folder_path/images/Flickr8k_Dataset.zip -d $folder_path/images\n",
    "    print('*** Descargando anotaciones de las imágenes...')\n",
    "    !wget http://hockenmaier.cs.illinois.edu/8k-pictures.html -P $folder_path/annotations -q --show-progress\n",
    "\n",
    "print('Inicializando pytorch Flickr8k dataset')\n",
    "full_flickr_set = torchvision.datasets.Flickr8k(root=f'{folder_path}/images/Flicker8k_Dataset', ann_file = f'{folder_path}/annotations/8k-pictures.html')\n",
    "\n",
    "print('Creando train, val y test splits...')\n",
    "train_flickr_set, val_flickr_set, test_flickr_set = [], [], []\n",
    "for i, item in enumerate(full_flickr_set):\n",
    "  if i<6000:\n",
    "    train_flickr_set.append(item)\n",
    "  elif i<7000:\n",
    "    val_flickr_set.append(item)\n",
    "  else:\n",
    "    test_flickr_set.append(item)\n",
    "\n",
    "print('Listo!')"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "*** Descargando y extrayendo Flickr8k, siéntese y relájese 4 mins...\n",
      "****** Descargando las imágenes...\n",
      "Flickr8k_Dataset.zi  96%[==================> ]   1.01G  3.65MB/s    in 3m 6s   \n",
      "Flickr8k_Dataset.zi 100%[+++++++++++++++++++>]   1.04G  5.92MB/s    in 6.6s    \n",
      "********* Extrayendo las imágenes...\n",
      "  Si te sale mensaje de colab, dale Ignorar\n",
      "\n",
      "*** Descargando anotaciones de las imágenes...\n",
      "8k-pictures.html    100%[===================>]   3.53M  6.69MB/s    in 0.5s    \n",
      "Inicializando pytorch Flickr8k dataset\n",
      "Creando train, val y test splits...\n",
      "Listo!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDKp92H8OnZB"
   },
   "source": [
    "#### Extrae los textos"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b2q2_dNcMda2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968787875,
     "user_tz": 180,
     "elapsed": 812,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "c31bd266-04ea-4680-c2f8-a2159b8d423d"
   },
   "source": [
    "train_text = extract_text_from_set(train_flickr_set)\n",
    "val_text = extract_text_from_set(val_flickr_set)\n",
    "test_text = extract_text_from_set(test_flickr_set)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 619588.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 688041.99it/s]\n",
      "100%|██████████| 663/663 [00:00<00:00, 162222.82it/s]\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5HI9-O0OqIT"
   },
   "source": [
    "#### Genera los tokens"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cFt8moUHNtII",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968795777,
     "user_tz": 180,
     "elapsed": 5301,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    }
   },
   "source": [
    "tokenizer = get_tokenizer('spacy')\n",
    "counter = Counter()  # para llevar la cuenta de los tokens y su ocurrencia\n",
    "\n",
    "train_tokens, counter = tokenize_text(train_text, tokenizer, counter)\n",
    "test_tokens, counter = tokenize_text(test_text, tokenizer, counter)\n",
    "val_tokens, counter = tokenize_text(val_text, tokenizer, counter)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hatUPnGOwdi"
   },
   "source": [
    "#### Define el vocabulario y agrega `<pad>` y `<sos>`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UTJ-7AXmOSOw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968799570,
     "user_tz": 180,
     "elapsed": 714,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    }
   },
   "source": [
    "vocab = list(counter.keys())\n",
    "vocab.append('<pad>')\n",
    "vocab.append('<sos>')\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "pad_idx = word2idx['<pad>']"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WikFbU1LPA_G"
   },
   "source": [
    "#### Convierte oraciones a ids y genera los dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zDcBWuesOhrJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968806676,
     "user_tz": 180,
     "elapsed": 760,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    }
   },
   "source": [
    "train_sentences = encode_sentences(train_tokens, vocab, word2idx)\n",
    "test_sentences = encode_sentences(test_tokens, vocab, word2idx)\n",
    "val_sentences = encode_sentences(val_tokens, vocab, word2idx)\n",
    "\n",
    "train_dataset = TextDataset(train_sentences)\n",
    "test_dataset = TextDataset(test_sentences)\n",
    "val_dataset = TextDataset(val_sentences)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSvpUdxURgLR"
   },
   "source": [
    "Con todo lo anterior, además de tener los dataset para entrenamiento, podemos también obtener identificadores correspondientes a textos que nosotros decidamos, haciendo algo como lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OCml7LsLPpNl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968820081,
     "user_tz": 180,
     "elapsed": 782,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "14f6ee4a-a581-4120-8b24-c53cd5c18540"
   },
   "source": [
    "s1 = 'A woman holding a cup of tea.'\n",
    "s2 = 'A man with a dog.'\n",
    "\n",
    "S = [s1,s2]\n",
    "tokens, _ = tokenize_text(S, tokenizer)\n",
    "D = encode_sentences(tokens, vocab, word2idx)\n",
    "\n",
    "print('tokens:', tokens)\n",
    "print('ids:', D)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "tokens: [['<sos>', 'a', 'woman', 'holding', 'a', 'cup', 'of', 'tea', '.'], ['<sos>', 'a', 'man', 'with', 'a', 'dog', '.']]\n",
      "ids: [[8460, 0, 238, 94, 0, 1570, 9, 1022, 14], [8460, 0, 78, 36, 0, 27, 14]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8oMGt5CpSNQ"
   },
   "source": [
    "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_LS1ADBtQscp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968834538,
     "user_tz": 180,
     "elapsed": 790,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    }
   },
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, \n",
    "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, \n",
    "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, \n",
    "    collate_fn=lambda data_list: pad_sequence_with_lengths(data_list, pad_idx))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mipSa7R1jAkd"
   },
   "source": [
    "**IMPORTANTE**: Nuestros datasets y dataloaders consideran también los largos de las secuencias. El siguiente código obtiene el primer elemento del dataset y el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es un par: la primera componente del par tiene los datos (los índices) mientras que la segunda componente tiene información de los largos de las secuencias."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FC-g5Ceyichj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968858230,
     "user_tz": 180,
     "elapsed": 685,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "088327aa-bb3c-4220-b7b9-c4bbefb4e535"
   },
   "source": [
    "d, length = test_dataset[0]\n",
    "print('len(d):', len(d))\n",
    "print('length:', length)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "len(d): 13\n",
      "length: 13\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e31bJzWwQsah",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1609968862058,
     "user_tz": 180,
     "elapsed": 718,
     "user": {
      "displayName": "Ignacio Andrés Slater Muñoz",
      "photoUrl": "",
      "userId": "09281725258070764127"
     }
    },
    "outputId": "8df31363-c0ae-4454-ddcf-035efab59345"
   },
   "source": [
    "# Obtiene un paquete desde el dataloader\n",
    "for data in test_dataloader:\n",
    "  D, Lengths = data\n",
    "  break\n",
    "\n",
    "print(D.size())\n",
    "print(Lengths.size())\n",
    "\n",
    "# La primera dimensión de D corresponde al largo\n",
    "# máximo de las secuencias en el batch\n",
    "assert D.size()[0] == torch.max(Lengths)\n",
    "\n",
    "# La segunda dimensión de D corresponde al tamaño del\n",
    "# batch, al igual que la dimensión de Lengths\n",
    "assert D.size()[1] == batch_size \n",
    "assert Lengths.size()[0] == batch_size"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "torch.Size([25, 64])\n",
      "torch.Size([64])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFRqoHLxrzYR"
   },
   "source": [
    "## 1a) Red recurrente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nZswYBYer0kz"
   },
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, LSTM, Linear, functional\n",
    "\n",
    "\n",
    "class RecurrentNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a basic recurrent neural network with autoregression for language modeling.\n",
    "    \"\"\"\n",
    "    __classifier: Linear\n",
    "    __lstm: LSTM\n",
    "\n",
    "    def __init__(self, vocab_size: int, output_size: int, embedding_dim: int, hidden_dim: int,\n",
    "                 n_layers: int, pad_idx: int, dropout: float = 0.5):\n",
    "        \"\"\"\n",
    "        Creates a new instance of the network following the GRU architecture and using an embedding\n",
    "        layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size:\n",
    "                the number of elements on the vocabulary.\n",
    "            output_size:\n",
    "                 the number of categories the network classifies.\n",
    "            embedding_dim:\n",
    "                the dimensions of the embeding layer.\n",
    "            hidden_dim:\n",
    "                the dimension of the initial hidden state.\n",
    "            n_layers:\n",
    "                the number of layers in the network.\n",
    "            pad_idx:\n",
    "                an index to represent the padding.\n",
    "        \"\"\"\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        # embedding = Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.__lstm = LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.__classifier = Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, nn_input: Tensor, h_0: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        out, h = self.__lstm(nn_input, h_0)\n",
    "        out = self.__classifier(functional.relu(out[:, -1]))\n",
    "        return out, h\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTKju5aPsA6H"
   },
   "source": [
    "## 1b) Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d0YB5e-NLQZC"
   },
   "source": [
    "# Acá tu código para el loop de entrenamiento\n",
    "# y los gráficos de la pérdida"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3fdjct4b8Pt"
   },
   "source": [
    "## 1c) Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mq_6_M_PcFQ0"
   },
   "source": [
    "# Acá tu código para generar texto usando el modelo\n",
    "\n",
    "def generate_sentence(model, init_sentence, ...):\n",
    "  # Usa acá lo que necesites para crear una secuencia de\n",
    "  # salida. Muy posiblemente tendrás que usar un tokenizador\n",
    "  # y el diccionario para pasar de índices a tokens (palabras).\n",
    "  sentence = None\n",
    "  return sentence"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxr7W07Rb_Zx"
   },
   "source": [
    "## 1d) Opcional: Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ELk5bZgycCFz"
   },
   "source": [
    "# Acá tu código para generar texto usando beam search\n",
    "\n",
    "def beam_search_generation(model, init_sentence, K, ...):\n",
    "  # El K representa al ancho del beam para la búsqueda.\n",
    "  return sentence"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mw8PKh-uF7P"
   },
   "source": [
    "# Parte 2 (Opcional): Subtitulado de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-jqTnsbSjpZ"
   },
   "source": [
    "#### Generamos transformación para el dataset\n",
    "\n",
    "Algo importante es que estamos usando la normalización estándar para los modelos pre-entrenados que provee pytoch. Si vas a usar algún otro modelo (o incluso uno generado por ti), podrías necesitar otra normalización. También nota que estamos usando el tamaño estándar de `224x224` para las imágenes que reciben los modelos pre-entrenados de pytorch. Si no quieres usar esos modelos o si quieres hacer el entrenamiento más rápido, puedes cambiarle la resolución a las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "97n_3naFlAO9"
   },
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "              transforms.ToTensor(), \n",
    "              transforms.Resize((224, 224)),\n",
    "              transforms.Normalize(\n",
    "                  mean=[0.485, 0.456, 0.406], \n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_HkFYB-sQ0t"
   },
   "source": [
    "#### Creamos los data loaders (puedes cambiar el tamaño del batch si lo deseas)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g1djUvkSpmlT"
   },
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    CaptioningDataset(\n",
    "        train_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    CaptioningDataset(\n",
    "        test_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
    "    )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    CaptioningDataset(\n",
    "        val_flickr_set, transform, tokenizer, word2idx, \"<sos>\", \".\"),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: pad_sequence_with_images(x, pad_idx)\n",
    "    )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHH-FpoZsVEE"
   },
   "source": [
    "**IMPORTANTE**: Nuestros dataloaders ahora contienen las secuencias de identificadores de los tokens del texto, los largos de las secuencias y las imágenes correspondientes. El siguiente código obtiene el primer elemento del dataloader de prueba. Nota que lo que entregan en ambos casos es una tripleta: la primera componente tiene los datos desde los textos (los índices), la segunda componente tiene información de los largos de las secuencias, y la tercera componente la información de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nTQQhZZWqPWF"
   },
   "source": [
    "# Obtiene un paquete desde el dataloader\n",
    "for data in test_dataloader:\n",
    "  Text, Lengths, Img = data\n",
    "  break\n",
    "\n",
    "print(Text.size())\n",
    "print(Lengths.size())\n",
    "print(Img.size())\n",
    "\n",
    "# La primera dimensión de Text corresponde al largo\n",
    "# máximo de las secuencias en el batch\n",
    "assert Text.size()[0] == torch.max(Lengths)\n",
    "\n",
    "# La segunda dimensión de D corresponde al tamaño del\n",
    "# batch, al igual que la dimensión de Lengths y la primera\n",
    "# dimensión de Img\n",
    "assert Text.size()[1] == batch_size \n",
    "assert Lengths.size()[0] == batch_size\n",
    "assert Img.size()[0] == batch_size"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-CnRQBOTQjw"
   },
   "source": [
    "### Usando modelos pre-entrenados\n",
    "\n",
    "El siguiente código carga VGG16 (pre-entrenado), pasa el modelo a la GPU. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M2i2IL4-s0PJ"
   },
   "source": [
    "import torchvision.models as models\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16 = vgg16.to('cuda')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_aVkA4tTr9H"
   },
   "source": [
    "Con un codigo como el siguiente podemos calcular las características para las imágenes `Img` del batch que obtuvimos más arriba. Nota el uso de `.eval()` y `with torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "StImMpdss0F8"
   },
   "source": [
    "Img = Img.to('cuda')\n",
    "\n",
    "vgg16.eval()\n",
    "with torch.no_grad():\n",
    "  F = vgg16.features(Img)\n",
    "\n",
    "print(F.size())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqi-CEqkuVM-"
   },
   "source": [
    "Finalmente y por si lo necesitas, puedes acceder a las imágenes originales del dataloader haciendo algo como esto:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D_y725BJs0UK"
   },
   "source": [
    "val_dataloader.dataset.original_image(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtaST3BqXOks"
   },
   "source": [
    "## 2a) Red convolucional + recurrente"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JTcuwtNEs0D1"
   },
   "source": [
    "class CaptioningModel(torch.nn.Module):\n",
    "    def __init__(self, ...): \n",
    "        # Crea las capas considerando una parte que procese debe procesar\n",
    "        # la imagen de entrada y otra que debe producir el texto (índices)\n",
    "        # de salida.\n",
    "        pass\n",
    "        \n",
    "    def forward(self, ...):\n",
    "        # Acá debes programar la pasada hacia adelante.\n",
    "        # Debes decidir qué le pasarás a la red y cómo haras la \n",
    "        # computación hacia adelante. Considera que no solo\n",
    "        # debes entrenar los parámetros sino que además debes\n",
    "        # después ser capaz de generar una secuencia de salida\n",
    "        # desde una imagen de entrada.\n",
    "        return ...   "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ifPKElXPk_"
   },
   "source": [
    "## 2b) Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Xb6B5CFs0AU"
   },
   "source": [
    "# Acá tu código para el loop de entrenamiento\n",
    "# y los gráficos de la pérdida"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYtj8aw4XQSX"
   },
   "source": [
    "## 2c) Generando texto desde imágenes de prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EYQ_LUEmsz97"
   },
   "source": [
    "# Acá tu código para generar texto usando desde imágenes\n",
    "# y un par de ejemplos con las imágenes del conjunto de prueba"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}